import os
import torch
import torch.nn as nn
from torchvision import transforms
from torchvision.datasets.folder import default_loader
from tqdm import tqdm
from model.repvgg import build_model
from utils import print_model_info
from config import config
from dataset import infer_channels
from model.repvgg import get_RepVGG_func_by_name
from PIL import Image

# --------------------------------------------------------
# RepVGG æ¨ç†è„šæœ¬
# æ”¯æŒå•å¼ å›¾ç‰‡æˆ–æ–‡ä»¶å¤¹æ‰¹é‡æ¨ç†ï¼Œè‡ªåŠ¨è¾“å‡ºç±»åˆ«å’Œç½®ä¿¡åº¦
# æ”¯æŒä¸è®­ç»ƒä¸€è‡´çš„é¢„å¤„ç†
# --------------------------------------------------------

def prepare_transform(image_size, channels):
    """æ¨ç†ç”¨transformï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰"""
    normalize_mean = [0.5] * channels
    normalize_std = [0.5] * channels

    return transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=normalize_mean, std=normalize_std)
    ])

def infer_single_image(model, image_path, transform):
    """å•å¼ å›¾ç‰‡æ¨ç†"""
    image = default_loader(image_path)  # PIL.Image
    image = transform(image).unsqueeze(0).to(config['device'])  # æ·»åŠ batchç»´åº¦
    model.eval()
    with torch.no_grad():
        output = model(image)
        probs = torch.softmax(output, dim=1)
        pred = torch.argmax(probs, dim=1).item()
        conf = probs[0][pred].item()
    return pred, conf

def infer_path(input_path):
    channels = infer_channels(config['train_dir'])
    transform = prepare_transform(config['image_size'], channels)

    func = get_RepVGG_func_by_name(config['model_name'])
    model = func(deploy=config['deploy'])
    model.linear = nn.Linear(model.linear.in_features, config['num_classes'])

    checkpoint = torch.load(config['pretrained_path'], map_location=config['device'])
    if config['deploy']:
        model.load_state_dict(checkpoint)
        print(f"âœ… [Deploy] Loaded deploy model from {config['pretrained_path']}")
    else:
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… [Train] Loaded training checkpoint from {config['pretrained_path']}")

    model.to(config['device'])
    model.eval()

    classes = config['class_names']  # æ˜¾å¼ç±»åˆ«ï¼ˆå¦‚ ['cat', 'dog']ï¼‰

    def infer_single_image(img_path):
        img = Image.open(img_path).convert('RGB' if channels == 3 else 'L')
        img = transform(img).unsqueeze(0).to(config['device'])
        with torch.no_grad():
            output = model(img)
            prob = torch.softmax(output, dim=1)  # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆsoftmaxæ¦‚ç‡ï¼‰
            conf, pred = torch.max(prob, 1)      # å–ç½®ä¿¡åº¦åŠç±»åˆ«ç´¢å¼•
            return classes[pred.item()], conf.item()  # è¿”å›ç±»åˆ«åä¸ç½®ä¿¡åº¦

    if os.path.isfile(input_path):
        cls, conf = infer_single_image(input_path)
        print(f"ğŸ–¼ï¸ {input_path} => é¢„æµ‹ç±»åˆ«: {cls} | ç½®ä¿¡åº¦: {conf:.4f}")
    elif os.path.isdir(input_path):
        img_paths = [os.path.join(input_path, img) for img in os.listdir(input_path)
                     if img.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
        print(f"ğŸ“‚ æ–‡ä»¶å¤¹æ¨ç†ï¼Œå…± {len(img_paths)} å¼ å›¾ç‰‡ï¼š")

        results = []  # æ”¶é›†æ¨ç†ç»“æœ
        for img_path in tqdm(img_paths, desc="æ¨ç†ä¸­", unit="img"):
            cls, conf = infer_single_image(img_path)
            results.append((os.path.basename(img_path), cls, conf))

        print("\nğŸ“‹ æ¨ç†ç»“æœï¼ˆå›¾ç‰‡å | ç±»åˆ« | ç½®ä¿¡åº¦ï¼‰ï¼š")
        print("=" * 60)
        for img_name, cls, conf in results:
            print(f"{img_name:30s}  ==>  {cls:15s} | ç½®ä¿¡åº¦: {conf:.4f}")
        print("=" * 60)
    else:
        raise ValueError(f"âŒ è¾“å…¥è·¯å¾„ {input_path} ä¸æ˜¯æœ‰æ•ˆçš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ï¼")